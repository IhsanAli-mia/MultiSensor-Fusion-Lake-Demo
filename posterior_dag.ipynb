{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import hashlib\n",
    "from datetime import datetime, timezone\n",
    "from typing import Union, List, Optional\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIO_ENDPOINT_URL = \"http://localhost:9000\" # Your MinIO endpoint\n",
    "MINIO_ACCESS_KEY = \"minioadmin\"\n",
    "MINIO_SECRET_KEY = \"minioadmin\"\n",
    "S3_BUCKET_NAME = \"fusion-lake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_options = {\n",
    "    \"key\": MINIO_ACCESS_KEY,\n",
    "    \"secret\": MINIO_SECRET_KEY,\n",
    "    \"client_kwargs\": {'endpoint_url': MINIO_ENDPOINT_URL},\n",
    "    \"config_kwargs\": {'s3': {'addressing_style': 'path'}} # Important for MinIO\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(**s3_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'stac/'  # Replace with your prefix\n",
    "stac_files = s3.ls(f's3://{S3_BUCKET_NAME}/{prefix}')\n",
    "stac_files = [f for f in stac_files if f.endswith('.json')]\n",
    "for obj in stac_files:\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_storage_options_for_intake_xr = s3_options.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize(arr):\n",
    "    return (arr - arr.min()) / (arr.max() - arr.min() + 1e-6)\n",
    "\n",
    "def compute_cloud_mask(chunk):\n",
    "    # Extract relevant bands\n",
    "    blue = chunk[1]\n",
    "    red = chunk[3]\n",
    "    nir = chunk[4]\n",
    "    swir1 = chunk[5]\n",
    "    thermal = chunk[9]\n",
    "\n",
    "    # Normalized indices\n",
    "    ndvi = (nir - red) / (nir + red + 1e-6)\n",
    "    \n",
    "    blue_n = normalize(blue)\n",
    "    swir1_n = normalize(swir1)\n",
    "\n",
    "    # Cloud condition\n",
    "    potential_cloud = (\n",
    "        (blue_n > 0.2) &\n",
    "        (swir1_n > 0.2) &\n",
    "        (ndvi < 0.3) &\n",
    "        (thermal < 30000)  \n",
    "    )\n",
    "\n",
    "    return potential_cloud.astype(np.uint8)\n",
    "\n",
    "def cloud_cover_posterior(arr):\n",
    "    \n",
    "    def process_block(block):\n",
    "        mask = compute_cloud_mask(block)\n",
    "        return np.array([[mask.mean()]], dtype='float32') \n",
    "\n",
    "    # Apply over chunks using map_blocks\n",
    "    cloud_fraction = arr.data.map_blocks(\n",
    "        lambda block: process_block(block),\n",
    "        dtype='float32',\n",
    "        drop_axis=0\n",
    "    )\n",
    "    \n",
    "    cloud_cover_da = xr.DataArray(\n",
    "    cloud_fraction,\n",
    "        dims=['chunk_y', 'chunk_x'],\n",
    "        name='cloud_fraction'\n",
    "    )\n",
    "    \n",
    "    # mean = cloud_cover_da.mean(dim=['chunk_y', 'chunk_x']).compute()\n",
    "    # variance = cloud_cover_da.var().compute()\n",
    "    \n",
    "    # print(f\"Mean cloud cover: {mean.item() * 100:.2f}%\")\n",
    "    # print(f\"Variance of cloud cover: {(variance.item() * 100**2):.2f} (% squared)\")\n",
    "    \n",
    "    # print(cloud_cover_da)\n",
    "    \n",
    "    return cloud_cover_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ledger(output_s3_bucket,summary_json_content,summary_data,scene_id_for_summary):\n",
    "    fs_for_ledger = s3\n",
    "    \n",
    "    ledger_bucket = output_s3_bucket\n",
    "    ledger_file_name = \"ledgers.csv\"\n",
    "    ledger_s3_key = f\"{ledger_file_name}\"\n",
    "    ledger_full_s3_path = f\"s3://{ledger_bucket}/{ledger_s3_key}\"\n",
    "\n",
    "    print(f\"Updating placeholder ledger at: {ledger_full_s3_path}\")\n",
    "\n",
    "    try:\n",
    "        summary_json_bytes = summary_json_content.encode('utf-8')\n",
    "        sha256_hash = hashlib.sha256(summary_json_bytes).hexdigest()\n",
    "        timestamp_utc_str = summary_data[\"calculation_timestamp_utc\"] # Use timestamp from summary\n",
    "\n",
    "        scene_id_ledger = summary_data[\"scene_id\"]\n",
    "        mean_val_ledger = summary_data[\"mean\"]\n",
    "        var_val_ledger = summary_data[\"variance\"]\n",
    "        stage_ledger = \"Cloud Posterior\"\n",
    "\n",
    "        new_ledger_line = f\"{scene_id_ledger},{sha256_hash},{timestamp_utc_str},{stage_ledger},{mean_val_ledger},{var_val_ledger}\\n\"\n",
    "\n",
    "        if fs_for_ledger.exists(ledger_s3_key):\n",
    "            print(f\"Ledger file {ledger_s3_key} exists, appending.\")\n",
    "            with fs_for_ledger.open(ledger_s3_key, \"ab\") as f:\n",
    "                f.write(new_ledger_line.encode(\"utf-8\"))\n",
    "        else:\n",
    "            print(f\"Ledger file {ledger_s3_key} does not exist, creating with header.\")\n",
    "            header = \"scene_id,summary_json_sha256,timestamp_utc,stage,mean,variance\\n\"\n",
    "            content = header + new_ledger_line\n",
    "            with fs_for_ledger.open(ledger_full_s3_path, \"wb\") as f:\n",
    "                f.write(content.encode(\"utf-8\"))\n",
    "        print(f\"Ledger updated successfully for placeholder scene {scene_id_ledger}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to update ledger for placeholder scene {scene_id_for_summary}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"\\nPlaceholder summary JSON and ledger update complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_json(cloud_cover_da,input_zarr_name,zarr_catalog_url,output_s3_path,output_s3_bucket):\n",
    "    scene_id_for_summary = input_zarr_name\n",
    "\n",
    "    mean = cloud_cover_da.mean(dim=['chunk_y', 'chunk_x']).compute()\n",
    "    variance = cloud_cover_da.var().compute()\n",
    "\n",
    "    summary_data = {\n",
    "        \"scene_id\": scene_id_for_summary,\n",
    "        \"processed_product\": \"NDVI\",\n",
    "        \"source_zarr\": zarr_catalog_url, # Link back to the input\n",
    "        \"output_ndvi_zarr\": output_s3_path,\n",
    "        \"mean\": float(mean.item()), # Ensure it's a standard float\n",
    "        \"variance\": float(variance.item()),\n",
    "        \"calculation_timestamp_utc\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "\n",
    "    summary_json_content = json.dumps(summary_data, indent=2)\n",
    "    print(f\"Summary JSON content: \\n{summary_json_content}\")\n",
    "\n",
    "    summary_json_s3_path = f\"s3://{output_s3_bucket}/posterior/ndvi_summary_{scene_id_for_summary}.json\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        print(f\"Uploading summary JSON to {summary_json_s3_path}...\")\n",
    "        with s3.open(summary_json_s3_path, 'wb') as f: # s3.open expects path within bucket\n",
    "            f.write(summary_json_content.encode('utf-8'))\n",
    "        print(\"Summary JSON uploaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading summary JSON: {e}\")\n",
    "        \n",
    "    update_ledger(output_s3_bucket,summary_json_content,summary_data,scene_id_for_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_zarr_json(cloud_cover_da,id,zarr_catalog_url):\n",
    "    input_zarr_name = id\n",
    "    output_s3_bucket = zarr_catalog_url.split('/')[2]\n",
    "    output_zarr_prefix = \"posterior\"\n",
    "    output_zarr_name = f\"{input_zarr_name}_cloud_cover.zarr\" \n",
    "    output_s3_path = f\"s3://{output_s3_bucket}/{output_zarr_prefix}/{output_zarr_name}\"\n",
    "\n",
    "    output_s3_map_root = f\"{output_s3_bucket}/{output_zarr_prefix}/{output_zarr_name}\"\n",
    "    if 's3' not in locals() or not isinstance(s3, s3fs.S3FileSystem):\n",
    "        s3 = s3fs.S3FileSystem(**s3_options)\n",
    "    s3_map_for_writing = s3fs.S3Map(root=output_s3_map_root, s3=s3, check=False)\n",
    "\n",
    "    # --- 5. Save the DataArray as a new Zarr store (Chunk-wise write) ---\n",
    "    cloud_ds_to_save = cloud_cover_da.to_dataset()\n",
    "    # print(cloud_ds_to_save)\n",
    "    print(\"Saving Zarr to S3...\")\n",
    "    try:\n",
    "        # task = cloud_ds_to_save.to_zarr(\n",
    "        #     store=s3_map_for_writing,\n",
    "        #     mode='w',\n",
    "        #     consolidated=True\n",
    "        # )\n",
    "        print(f\" Zarr store successfully written to {output_s3_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving  Zarr to S3: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    \n",
    "    save_json(cloud_cover_da,input_zarr_name,zarr_catalog_url,output_s3_path,output_s3_bucket)\n",
    "\n",
    "    print(\"\\nCloud cover calculation and Zarr upload complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_posterior(cloud_cover_da,id,zarr_catalog_url):\n",
    "    save_zarr_json(cloud_cover_da,id,zarr_catalog_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in stac_files:\n",
    "    stac_catalog_url = f's3://{obj}'\n",
    "    \n",
    "    with s3.open(stac_catalog_url, 'r') as f:\n",
    "        stac_item_dict = json.load(f)\n",
    "    print(f\"Successfully read JSON content from {stac_catalog_url}\")\n",
    "    \n",
    "    # print(stac_item_dict['assets'])\n",
    "    zarr_link = stac_item_dict['assets']['data_zarr']['href']\n",
    "    zarr_catalog_url = f's3://{S3_BUCKET_NAME}/{zarr_link}'\n",
    "    \n",
    "    # print(f\"Zarr catalog URL: {zarr_catalog_url}\")\n",
    "    \n",
    "    ds_lazy = xr.open_zarr(\n",
    "        store=zarr_catalog_url, # Path to the S3 Zarr store root\n",
    "        storage_options=s3_options,\n",
    "        consolidated=True,\n",
    "        chunks={}\n",
    "    )\n",
    "    print(\"Loaded Zarr dataset (lazy)\")\n",
    "    \n",
    "    id = stac_item_dict['id']\n",
    "    \n",
    "    arr = ds_lazy[id]\n",
    "    \n",
    "    cloud_cover_da = cloud_cover_posterior(arr)\n",
    "    print(\"Posterior Calculation Done\")\n",
    "    \n",
    "    save_posterior(cloud_cover_da,id,zarr_catalog_url)\n",
    "    \n",
    "    print('--------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputerVision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
