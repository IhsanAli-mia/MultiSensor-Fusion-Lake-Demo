{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr = ds_lazy['LC09_L1TP_141047_20250519_20250519_02_T1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize(arr):\n",
    "#     return (arr - arr.min()) / (arr.max() - arr.min() + 1e-6)\n",
    "\n",
    "# def compute_cloud_mask(chunk):\n",
    "#     # Extract relevant bands\n",
    "#     blue = chunk[1]\n",
    "#     red = chunk[3]\n",
    "#     nir = chunk[4]\n",
    "#     swir1 = chunk[5]\n",
    "#     thermal = chunk[9]\n",
    "\n",
    "#     # Normalized indices\n",
    "#     ndvi = (nir - red) / (nir + red + 1e-6)\n",
    "    \n",
    "#     blue_n = normalize(blue)\n",
    "#     swir1_n = normalize(swir1)\n",
    "\n",
    "#     # Cloud condition\n",
    "#     potential_cloud = (\n",
    "#         (blue_n > 0.2) &\n",
    "#         (swir1_n > 0.2) &\n",
    "#         (ndvi < 0.3) &\n",
    "#         (thermal < 30000)  \n",
    "#     )\n",
    "\n",
    "#     return potential_cloud.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_block(block):\n",
    "#     mask = compute_cloud_mask(block)\n",
    "#     return np.array([[mask.mean()]], dtype='float32') \n",
    "\n",
    "#     # Apply over chunks using map_blocks\n",
    "# cloud_fraction = arr.data.map_blocks(\n",
    "#     lambda block: process_block(block),\n",
    "#     dtype='float32',\n",
    "#     drop_axis=0,  # drop 'band' axis\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(cloud_fraction)\n",
    "# cloud_cover_da = xr.DataArray(\n",
    "#     cloud_fraction,\n",
    "#     dims=['chunk_y', 'chunk_x'],\n",
    "#     name='cloud_fraction'\n",
    "# )\n",
    "# print(cloud_cover_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = cloud_cover_da.mean(dim=['chunk_y', 'chunk_x']).compute()\n",
    "variance = cloud_cover_da.var().compute()\n",
    "\n",
    "print(f\"Mean cloud cover: {mean.item() * 100:.2f}%\")\n",
    "print(f\"Variance of cloud cover: {(variance.item() * 100**2):.2f} (% squared)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_cloud_detection_bands_v2(\n",
    "    scene_da: xr.DataArray,\n",
    "    blue_band_selector: Union[str, int],\n",
    "    nir_band_selector: Union[str, int],\n",
    "    # Provide the order of band names as they exist in your Zarr file's 'band' dimension\n",
    "    # This is only needed if you pass string selectors (e.g., 'blue', 'nir')\n",
    "    band_names_in_data: Optional[List[str]] = None,\n",
    "    red_band_selector: Optional[Union[str, int]] = None,\n",
    "    blue_reflectance_threshold: float = 0.25,\n",
    "    nir_reflectance_threshold: float = 0.20,\n",
    "    epsilon: float = 1e-8\n",
    ") -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Cloud detection operating on selected bands, robust to Zarr files\n",
    "    that only have dimension names but not band coordinate labels.\n",
    "    \"\"\"\n",
    "    print(f\"--- Inside simple_cloud_detection_bands_v2 ---\")\n",
    "    print(f\"Input scene_da: {scene_da.name}, dims: {scene_da.dims}, shape: {scene_da.shape}, chunks: {scene_da.chunks}\")\n",
    "    if 'band' not in scene_da.dims:\n",
    "        raise ValueError(f\"Input DataArray must have a 'band' dimension. Found dims: {scene_da.dims}\")\n",
    "\n",
    "    # Helper to get band index\n",
    "    def get_band_index(selector: Union[str, int], dim_name: str = 'band') -> int:\n",
    "        if isinstance(selector, int):\n",
    "            return selector\n",
    "        elif isinstance(selector, str):\n",
    "            if band_names_in_data is None:\n",
    "                raise ValueError(\n",
    "                    \"band_names_in_data must be provided if band selectors are strings \"\n",
    "                    \"and the DataArray does not have named band coordinates.\"\n",
    "                )\n",
    "            try:\n",
    "                return band_names_in_data.index(selector)\n",
    "            except ValueError:\n",
    "                raise ValueError(\n",
    "                    f\"Band name '{selector}' not found in band_names_in_data: {band_names_in_data}\"\n",
    "                )\n",
    "        else:\n",
    "            raise TypeError(f\"Band selector must be str or int, got {type(selector)}\")\n",
    "\n",
    "    # --- 1. Select Bands using integer positions ---\n",
    "    try:\n",
    "        blue_idx = get_band_index(blue_band_selector)\n",
    "        blue_band = scene_da.isel(band=blue_idx)\n",
    "        print(f\"Selected Blue band (selector: '{blue_band_selector}', index: {blue_idx})\")\n",
    "\n",
    "        nir_idx = get_band_index(nir_band_selector)\n",
    "        nir_band = scene_da.isel(band=nir_idx)\n",
    "        print(f\"Selected NIR band (selector: '{nir_band_selector}', index: {nir_idx})\")\n",
    "\n",
    "        red_band = None\n",
    "        if red_band_selector is not None:\n",
    "            red_idx = get_band_index(red_band_selector)\n",
    "            red_band = scene_da.isel(band=red_idx)\n",
    "            print(f\"Selected Red band (selector: '{red_band_selector}', index: {red_idx})\")\n",
    "\n",
    "    except IndexError as e:\n",
    "        print(f\"Error during band selection: {e}\")\n",
    "        print(f\"Max index for 'band' dimension (size-1): {scene_da.sizes.get('band', 0) - 1}\")\n",
    "        print(f\"Attempted blue_idx: {blue_idx if 'blue_idx' in locals() else 'N/A'}, \"\n",
    "              f\"nir_idx: {nir_idx if 'nir_idx' in locals() else 'N/A'}\")\n",
    "        raise\n",
    "\n",
    "    # --- 2. Placeholder Cloud Logic ---\n",
    "    print(f\"Applying blue_reflectance_threshold: {blue_reflectance_threshold}\")\n",
    "    print(f\"Applying nir_reflectance_threshold: {nir_reflectance_threshold}\")\n",
    "\n",
    "    potential_clouds = (blue_band > blue_reflectance_threshold) & \\\n",
    "                       (nir_band > nir_reflectance_threshold)\n",
    "\n",
    "    potential_clouds = potential_clouds.rename(\"cloud_mask_placeholder\")\n",
    "    # ... (add attributes as before)\n",
    "    print(f\"--- Exiting simple_cloud_detection_bands_v2 ---\")\n",
    "    return potential_clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDERED_BAND_NAMES_IN_FILE = ['blue_channel', 'green_channel', 'near_infrared', 'red_channel',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCalling simple_cloud_detection_bands_v2...\")\n",
    "cloud_mask_da = simple_cloud_detection_bands_v2(\n",
    "    scene_da=data_array_to_process,\n",
    "    blue_band_selector='blue_channel',    # String name\n",
    "    nir_band_selector='near_infrared', # String name\n",
    "    band_names_in_data=ORDERED_BAND_NAMES_IN_FILE, # Crucial mapping\n",
    "    blue_reflectance_threshold=0.25,\n",
    "    nir_reflectance_threshold=0.20\n",
    ")\n",
    "print(f\"\\nOutput Cloud Mask DataArray (cloud_mask_da) (lazy Dask array):\")\n",
    "print(cloud_mask_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCalculating cloud cover percentage (triggers Dask computation)...\")\n",
    "# Ensure we only average over spatial dimensions if the mask is already 2D\n",
    "if len(cloud_mask_da.dims) == 2 and 'y' in cloud_mask_da.dims and 'x' in cloud_mask_da.dims:\n",
    "    cloud_proportion = cloud_mask_da.mean(dim=['y', 'x'])\n",
    "else: # A more general mean if dimensions are different, though not expected here\n",
    "    cloud_proportion = cloud_mask_da.mean()\n",
    "cloud_percentage = cloud_proportion.compute() * 100.0\n",
    "print(f\"\\nEstimated Cloud Cover Percentage: {cloud_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_variable_name = 'LC08_L1TP_032037_20160420_20170223_01_T1'\n",
    "data_array_to_process = ds_lazy[data_variable_name]\n",
    "\n",
    "# 2. Define your cloud posterior function (example from before)\n",
    "# This function expects a 1D numpy array of spectral values for a single pixel\n",
    "def simple_cloud_posterior(pixel_spectral_bands, bright_threshold=0.6, whiteness_factor=0.15):\n",
    "    \"\"\"\n",
    "    Calculate a simple cloud posterior probability for a single pixel.\n",
    "    Args:\n",
    "        pixel_spectral_bands (np.ndarray): 1D array of spectral values (e.g., [R, G, B]).\n",
    "                                          Assumed to be scaled roughly 0-1.\n",
    "        bright_threshold (float): Pixels with mean reflectance below this are considered clear.\n",
    "        whiteness_factor (float): How \"white\" a pixel needs to be.\n",
    "    Returns:\n",
    "        float: A pseudo-posterior probability (0.0 to 1.0) of the pixel being cloud.\n",
    "    \"\"\"\n",
    "    if not isinstance(pixel_spectral_bands, np.ndarray): # Should be numpy array from dask chunk\n",
    "        pixel_spectral_bands = np.array(pixel_spectral_bands)\n",
    "\n",
    "    mean_reflectance = np.mean(pixel_spectral_bands)\n",
    "\n",
    "    if mean_reflectance < bright_threshold:\n",
    "        return 0.0  # Definitely not cloud if not bright enough\n",
    "\n",
    "    if mean_reflectance > 1e-6:\n",
    "        color_variation = np.std(pixel_spectral_bands) / mean_reflectance\n",
    "    else:\n",
    "        color_variation = 1.0\n",
    "\n",
    "    brightness_score = min(1.0, max(0.0, (mean_reflectance - bright_threshold) / (1.0 - bright_threshold + 1e-6)))\n",
    "    whiteness_score = max(0.0, 1.0 - (color_variation / whiteness_factor))\n",
    "    cloud_prob = brightness_score * whiteness_score\n",
    "    return float(cloud_prob)\n",
    "\n",
    "\n",
    "# 3. Apply the function chunkwise using xarray.apply_ufunc\n",
    "cloud_posterior_da = xr.apply_ufunc(\n",
    "    simple_cloud_posterior,         # The function to apply\n",
    "    data_array_to_process,          # The input DataArray (NOT the Dataset)\n",
    "    input_core_dims=[['x']],        # The dimension your function operates on is 'x'\n",
    "    output_core_dims=[[]],          # Your function returns a scalar\n",
    "    exclude_dims=set(('x',)),       # The 'x' dimension is consumed by the function\n",
    "    dask=\"parallelized\",            # Enable Dask parallelization\n",
    "    output_dtypes=[float],          # The data type of the output\n",
    "    kwargs={'bright_threshold': 0.65, 'whiteness_factor': 0.2} # Pass additional args\n",
    ")\n",
    "\n",
    "# The resulting cloud_posterior_da will have dimensions ('band', 'y')\n",
    "# because the 'x' dimension was consumed.\n",
    "cloud_posterior_da.name = 'cloud_posterior' # Give it a name\n",
    "\n",
    "print(\"Input DataArray to process:\")\n",
    "print(data_array_to_process)\n",
    "print(\"\\nCloud Posterior DataArray structure:\")\n",
    "print(cloud_posterior_da)\n",
    "print(\"\\nCloud Posterior DataArray Chunks:\")\n",
    "print(cloud_posterior_da.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Inspecting cloud_posterior_da ---\")\n",
    "print(repr(cloud_posterior_da)) # Full representation\n",
    "print(\"\\nShape:\", cloud_posterior_da.shape)\n",
    "print(\"Dimensions:\", cloud_posterior_da.dims)\n",
    "print(\"Chunks:\", cloud_posterior_da.chunks)\n",
    "print(\"Dtype:\", cloud_posterior_da.dtype)\n",
    "\n",
    "# Important: Inspect the _meta attribute of the underlying Dask array\n",
    "if hasattr(cloud_posterior_da, 'data') and hasattr(cloud_posterior_da.data, '_meta'):\n",
    "    print(\"Dask array _meta:\", repr(cloud_posterior_da.data._meta))\n",
    "else:\n",
    "    print(\"Could not access Dask array _meta.\")\n",
    "\n",
    "# Try to compute a small part to see the actual values in a chunk\n",
    "print(\"\\nComputing a small slice of cloud_posterior_da\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud_posterior_values = cloud_posterior_da.values\n",
    "\n",
    "# These are still Dask arrays. Compute() triggers calculation.\n",
    "mean_cloud_cover = cloud_posterior_da.mean(skipna=True)\n",
    "variance_cloud_cover = cloud_posterior_da.var(skipna=True)\n",
    "\n",
    "print(\"Computing Cloud mean...\")\n",
    "cloud_mean_computed = mean_cloud_cover.compute()\n",
    "print(f\"Computed cloud Mean: {cloud_mean_computed.item()}\") # .item() to get Python scalar\n",
    "\n",
    "print(\"Computing cloud variance...\")\n",
    "cloud_variance_computed = variance_cloud_cover.compute()\n",
    "print(f\"Computed cloud Variance: {cloud_variance_computed.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RED_BAND_INDEX_IN_X_DIM = 0\n",
    "NIR_BAND_INDEX_IN_X_DIM = 1\n",
    "DATA_VARIABLE_NAME = 'LC08_L1TP_050024_20160520_20170324_01_T1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_band_da = ds_lazy[DATA_VARIABLE_NAME]\n",
    "print(f\"Extracted multi-band DataArray ('{multi_band_da.name}'): \\n{multi_band_da}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Selecting Red band using index {RED_BAND_INDEX_IN_X_DIM} along 'x' dimension...\")\n",
    "# .isel(x=...) selects by integer position along the 'x' dimension.\n",
    "red_da = multi_band_da.isel(x=RED_BAND_INDEX_IN_X_DIM)\n",
    "# Rename dimensions for clarity if needed, e.g., if 'band' is height and 'y' is width\n",
    "# red_da = red_da.rename({'band': 'height', 'y': 'width'})\n",
    "\n",
    "\n",
    "print(f\"Selecting NIR band using index {NIR_BAND_INDEX_IN_X_DIM} along 'x' dimension...\")\n",
    "nir_da = multi_band_da.isel(x=NIR_BAND_INDEX_IN_X_DIM)\n",
    "# nir_da = nir_da.rename({'band': 'height', 'y': 'width'}) # Keep dimensions consistent with red_da\n",
    "\n",
    "print(f\"Red band selected (lazy): \\n{red_da}\")\n",
    "print(f\"NIR band selected (lazy): \\n{nir_da}\")\n",
    "\n",
    "# --- 3. Calculate NDVI (Chunk-wise operation) ---\n",
    "epsilon = 1e-8\n",
    "print(\"Calculating NDVI (lazy Dask computation)...\")\n",
    "ndvi_da = (nir_da - red_da) / (nir_da + red_da + epsilon)\n",
    "ndvi_da = ndvi_da.rename(\"ndvi\")\n",
    "\n",
    "ndvi_da.attrs.update({\n",
    "    'long_name': 'Normalized Difference Vegetation Index',\n",
    "    'units': '1',\n",
    "    'formula': '(NIR - Red) / (NIR + Red)',\n",
    "    'source_data_variable': DATA_VARIABLE_NAME,\n",
    "    'red_band_source_index_in_x_dim': RED_BAND_INDEX_IN_X_DIM,\n",
    "    'nir_band_source_index_in_x_dim': NIR_BAND_INDEX_IN_X_DIM,\n",
    "    'epsilon_for_division': epsilon\n",
    "})\n",
    "\n",
    "print(f\"NDVI DataArray (lazy): \\n{ndvi_da}\")\n",
    "print(f\"NDVI Chunks: {ndvi_da.chunks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating NDVI mean and variance (lazy Dask computation)...\")\n",
    "\n",
    "# Calculate mean, explicitly skipping NaNs if they might exist\n",
    "# If your data is clean (no NaNs expected), skipna=False might be slightly faster.\n",
    "ndvi_mean_lazy = ndvi_da.mean(skipna=True)\n",
    "ndvi_variance_lazy = ndvi_da.var(skipna=True)\n",
    "\n",
    "# We need to compute these values to put them in the JSON.\n",
    "# This will trigger reading the necessary data for the whole ndvi_da array.\n",
    "# If ndvi_da is very large, computing global mean/variance can be memory intensive\n",
    "# if not chunked optimally for reduction. Dask handles this reasonably well.\n",
    "print(\"Computing NDVI mean...\")\n",
    "ndvi_mean_computed = ndvi_mean_lazy.compute()\n",
    "print(f\"Computed NDVI Mean: {ndvi_mean_computed.item()}\") # .item() to get Python scalar\n",
    "\n",
    "print(\"Computing NDVI variance...\")\n",
    "ndvi_variance_computed = ndvi_variance_lazy.compute()\n",
    "print(f\"Computed NDVI Variance: {ndvi_variance_computed.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_zarr_name = DATA_VARIABLE_NAME # Using the main variable name\n",
    "output_s3_bucket = zarr_catalog_url.split('/')[2]\n",
    "output_zarr_prefix = \"posterior\"\n",
    "output_ndvi_zarr_name = f\"{input_zarr_name}_ndvi.zarr\" # Might want a cleaner name\n",
    "output_ndvi_s3_path = f\"s3://{output_s3_bucket}/{output_zarr_prefix}/{output_ndvi_zarr_name}\"\n",
    "print(f\"Output NDVI Zarr will be saved to: {output_ndvi_s3_path}\")\n",
    "\n",
    "output_s3_map_root = f\"{output_s3_bucket}/{output_zarr_prefix}/{output_ndvi_zarr_name}\"\n",
    "if 's3' not in locals() or not isinstance(s3, s3fs.S3FileSystem):\n",
    "    s3 = s3fs.S3FileSystem(**s3_options)\n",
    "s3_map_for_writing = s3fs.S3Map(root=output_s3_map_root, s3=s3, check=False)\n",
    "\n",
    "# --- 5. Save the NDVI DataArray as a new Zarr store (Chunk-wise write) ---\n",
    "ndvi_ds_to_save = ndvi_da.to_dataset()\n",
    "print(\"Saving NDVI Zarr to S3...\")\n",
    "try:\n",
    "    task = ndvi_ds_to_save.to_zarr(\n",
    "        store=s3_map_for_writing,\n",
    "        mode='w',\n",
    "        consolidated=True,\n",
    "        compute=True\n",
    "    )\n",
    "    print(f\"NDVI Zarr store successfully written to {output_ndvi_s3_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving NDVI Zarr to S3: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(\"\\nNDVI calculation and Zarr upload complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_id_for_summary = input_zarr_name\n",
    "\n",
    "summary_data = {\n",
    "    \"scene_id\": scene_id_for_summary,\n",
    "    \"processed_product\": \"NDVI\",\n",
    "    \"source_zarr\": zarr_catalog_url, # Link back to the input\n",
    "    \"output_ndvi_zarr\": output_ndvi_s3_path,\n",
    "    \"mean\": float(ndvi_mean_computed.item()), # Ensure it's a standard float\n",
    "    \"variance\": float(ndvi_variance_computed.item()),\n",
    "    \"calculation_timestamp_utc\": datetime.now(timezone.utc).isoformat()\n",
    "}\n",
    "\n",
    "summary_json_content = json.dumps(summary_data, indent=2)\n",
    "print(f\"Summary JSON content: \\n{summary_json_content}\")\n",
    "\n",
    "summary_json_s3_path = f\"s3://{output_s3_bucket}/posterior/ndvi_summary_{scene_id_for_summary}.json\"\n",
    "\n",
    "\n",
    "try:\n",
    "    print(f\"Uploading summary JSON to {summary_json_s3_path}...\")\n",
    "    with s3.open(summary_json_s3_path, 'wb') as f: # s3.open expects path within bucket\n",
    "        f.write(summary_json_content.encode('utf-8'))\n",
    "    print(\"Summary JSON uploaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading summary JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_for_ledger = s3\n",
    "\n",
    "ledger_bucket = output_s3_bucket\n",
    "ledger_file_name = \"ledgers.csv\"\n",
    "ledger_s3_key = f\"{ledger_file_name}\"\n",
    "ledger_full_s3_path = f\"s3://{ledger_bucket}/{ledger_s3_key}\"\n",
    "\n",
    "print(f\"Updating placeholder ledger at: {ledger_full_s3_path}\")\n",
    "\n",
    "try:\n",
    "    summary_json_bytes = summary_json_content.encode('utf-8')\n",
    "    sha256_hash = hashlib.sha256(summary_json_bytes).hexdigest()\n",
    "    timestamp_utc_str = summary_data[\"calculation_timestamp_utc\"] # Use timestamp from summary\n",
    "\n",
    "    scene_id_ledger = summary_data[\"scene_id\"]\n",
    "    mean_val_ledger = summary_data[\"mean\"]\n",
    "    var_val_ledger = summary_data[\"variance\"]\n",
    "    stage_ledger = \"NDVI_Posterior_Placeholder\"\n",
    "\n",
    "    new_ledger_line = f\"{scene_id_ledger},{sha256_hash},{timestamp_utc_str},{stage_ledger},{mean_val_ledger},{var_val_ledger}\\n\"\n",
    "\n",
    "    if fs_for_ledger.exists(ledger_s3_key):\n",
    "        print(f\"Ledger file {ledger_s3_key} exists, appending.\")\n",
    "        with fs_for_ledger.open(ledger_s3_key, \"ab\") as f:\n",
    "            f.write(new_ledger_line.encode(\"utf-8\"))\n",
    "    else:\n",
    "        print(f\"Ledger file {ledger_s3_key} does not exist, creating with header.\")\n",
    "        header = \"scene_id,summary_json_sha256,timestamp_utc,stage,mean,variance\\n\"\n",
    "        content = header + new_ledger_line\n",
    "        with fs_for_ledger.open(ledger_full_s3_path, \"wb\") as f:\n",
    "            f.write(content.encode(\"utf-8\"))\n",
    "    print(f\"Ledger updated successfully for placeholder scene {scene_id_ledger}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to update ledger for placeholder scene {scene_id_for_summary}: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\nPlaceholder summary JSON and ledger update complete.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
